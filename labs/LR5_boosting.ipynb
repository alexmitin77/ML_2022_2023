{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа - Бустинг, бэггинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## О задании\n",
    "\n",
    "В этом задании вам предстоит вручную запрограммировать один из самых мощных алгоритмов машинного обучения — бустинг. Работать мы будем на двух наборах данных: многомерных данных по кредитам с kaggle и синтетических двумерных. В данных с kaggle целевая переменная показывает, вернуло ли кредит физическое лицо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget  -O 'bank_data.csv' -q 'https://www.dropbox.com/s/uy27mctxo0gbuof/bank_data.csv?dl=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank_data.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим на train и test (random_state не меняем)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем синтетические данные (seed не меняем)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 10 ** 5\n",
    "num_thresholds = 50\n",
    "\n",
    "X_synthetic = np.random.normal(scale=3, size=[num_obs, 2])\n",
    "x1_thresholds = np.random.choice(X_synthetic[:, 0], num_thresholds, False)\n",
    "x2_thresholds = np.random.choice(X_synthetic[:, 1], num_thresholds, False)\n",
    "\n",
    "gains = np.random.uniform(-0.4086, 0.5, size=[2 * num_thresholds, 1])\n",
    "x1_thresholds_cond = [X_synthetic[:, 0] >= threshold for threshold in x1_thresholds]\n",
    "x2_thresholds_cond = [X_synthetic[:, 1] >= threshold for threshold in x2_thresholds]\n",
    "\n",
    "noise = np.random.uniform(-0.5, 0.5, size=num_obs)\n",
    "\n",
    "y_synthetic_probits = np.sum(\n",
    "    gains[:num_thresholds] * x1_thresholds_cond + gains[num_thresholds:] * x2_thresholds_cond, axis=0\n",
    ") + noise\n",
    "y_synthetic = np.sign(y_synthetic_probits)\n",
    "\n",
    "X_train_synthetic, y_train_synthetic = X_synthetic[:int(num_obs * 0.8)], y_synthetic[:int(num_obs * 0.8)]\n",
    "X_test_synthetic, y_test_synthetic = X_synthetic[int(num_obs * 0.8):], y_synthetic[int(num_obs * 0.8):]\n",
    "\n",
    "px.histogram(x = y_synthetic_probits, nbins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторый полезный код для визуализации предсказаний (пригодится позже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predicts(model, features, targets, x_lim=[-15.0, 15.0], y_lim=[-15.0, 15.0],\n",
    "                  examples_density=0.01, steps=1000, num_ticks=6, title='', mode='classification'):\n",
    "    '''\n",
    "    Функция для визуализации предсказаний модели на двухмерной плоскости\n",
    "    param model: обученная модель классификации или регрессии для двухмерных объектов\n",
    "    param features: признаки выборки (a.k.a. X)\n",
    "    param targets: целевая переменная выборки (a.k.a y)\n",
    "    param x_lim: пределы для x\n",
    "    param y_lim: пределы для y\n",
    "    param examples_density: доля выборки, которая будет нарисована\n",
    "    param steps: частота разбиения плоскости\n",
    "    param num_ticks: число подписей на графике\n",
    "    param title: заголовок графика\n",
    "    param mode: режим 'classification' - вероятности положительного класса\n",
    "                режим 'regression' - вещественная целевая переменная\n",
    "    '''\n",
    "    \n",
    "    mask = np.random.choice([True, False], size=features.shape[0], \n",
    "                            p=[examples_density, 1.0 - examples_density])\n",
    "    features_x = (features[mask, 0] - x_lim[0]) / (x_lim[1] - x_lim[0]) * steps\n",
    "    features_y = (features[mask, 1] - y_lim[0]) / (y_lim[1] - y_lim[0]) * steps\n",
    "    \n",
    "    xs = np.linspace(x_lim[0], x_lim[1], steps)\n",
    "    ys = np.linspace(y_lim[0], y_lim[1], steps)\n",
    "    \n",
    "    xs, ys = np.meshgrid(xs, ys)\n",
    "    grid = np.stack([xs.flatten(), ys.flatten()], axis=1)\n",
    "    if mode == 'classification':\n",
    "        predicts = model.predict_proba(grid)[:, 1].reshape(steps, steps)\n",
    "        values = (targets[mask] == 1).astype(np.float)\n",
    "    elif mode == 'regression':\n",
    "        predicts = model.predict(grid).reshape(steps, steps)\n",
    "        values = targets[mask]\n",
    "    else:\n",
    "        raise ValueError('Unknown mode')\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(predicts, origin='lower')\n",
    "    plt.scatter(features_x, features_y, c=values, edgecolors='white', linewidths=1.5)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.xticks(np.linspace(0, steps, num_ticks), np.linspace(x_lim[0], x_lim[1], num_ticks))\n",
    "    plt.yticks(np.linspace(0, steps, num_ticks), np.linspace(y_lim[0], y_lim[1], num_ticks))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Реализуйте бустинг для задачи бинарной классификации.\n",
    "\n",
    "Поскольку градиентный бустинг обучается через последовательное создание моделей, может получиться так, что оптимальная с точки зрения генерализации модель будет получена на промежуточной итерации. Обычно для контроля такого поведения в методе `fit` передается также валидационная выборка, по которой можно оценивать общее качество модели в процессе обучения (желательно делать это каждую итерацию, но если ваша имплементация слишком медленная или ваше железо не тянет, можно делать это реже). Кроме того, нет смысла обучать действительно глубокую модель на 1000 деревьев и больше, если оптимальный ансамбль получился, к примеру, на 70 итерации и в течение какого-то количества итераций не улучшился - поэтому мы также задействуем early stopping при отсутствии улучшений в течение некоторого числа итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boosting:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_class=DecisionTreeRegressor,\n",
    "        base_model_params: dict={'max_features': 0.1},\n",
    "        n_estimators: int=10,\n",
    "        learning_rate: float=0.1,\n",
    "        subsample: float=0.3,\n",
    "        random_seed: int=228,\n",
    "        custom_loss: list or tuple=None,\n",
    "        use_best_model: bool=False,\n",
    "        n_iter_early_stopping: int=None\n",
    "    ):\n",
    "        \n",
    "        # Класс базовой модели\n",
    "        self.base_model_class = base_model_class\n",
    "        # Параметры для инициализации базовой модели\n",
    "        self.base_model_params = base_model_params\n",
    "        # Число базовых моделей\n",
    "        self.n_estimators = n_estimators\n",
    "        # Длина шага (которая в лекциях обозначалась через eta)\n",
    "        self.learning_rate = learning_rate\n",
    "        # Доля объектов, на которых обучается каждая базовая модель\n",
    "        self.subsample = subsample\n",
    "        # seed для бутстрапа, если хотим воспроизводимость модели\n",
    "        self.random_seed = random_seed\n",
    "        # Использовать ли при вызове predict и predict_proba лучшее\n",
    "        # с точки зрения валидационной выборки число деревьев в композиции\n",
    "        self.use_best_model = use_best_model\n",
    "        # число итераций, после которых при отсутствии улучшений на валидационной выборке обучение завершается\n",
    "        self.n_iter_early_stopping = n_iter_early_stopping\n",
    "        \n",
    "        # Плейсхолдер для нулевой модели\n",
    "        self.initial_model_pred = None\n",
    "        \n",
    "        # Список для хранения весов при моделях\n",
    "        self.gammas = []\n",
    "        \n",
    "        # Создаем список базовых моделей\n",
    "        self.models = [self.base_model_class(**self.base_model_params) for _ in range(self.n_estimators)]\n",
    "        \n",
    "        # Если используем свою функцию потерь, ее нужно передать как список из loss-a и его производной\n",
    "        if custom_loss is not None:\n",
    "            self.loss_fn, self.loss_derivative = custom_loss\n",
    "        else:\n",
    "            self.sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "            self.loss_fn = lambda y, z: -np.log(self.sigmoid(y * z)).mean()\n",
    "            self.loss_derivative = lambda y, z: -y * self.sigmoid(-y * z)\n",
    "        \n",
    "        \n",
    "    def _fit_new_model(self, X: np.ndarray, y: np.ndarray or list, n_model: int):\n",
    "        \"\"\"\n",
    "        Функция для обучения одной базовой модели бустинга\n",
    "        :param X: матрица признаков\n",
    "        :param y: вектор целевой переменной\n",
    "        :param n_model: номер модели, которую хотим обучить\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code is here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "        \n",
    "    def _fit_initial_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Функция для построения нулевой (простой) модели. Не забудьте взять логарифм, потому что сигмоида применяется\n",
    "        уже к сумме предсказаний базовых моделей, а не к каждому предсказанию каждой модели по отдельности.\n",
    "        Подойдёт константная модель, возвращающая самый популярный класс,\n",
    "        но если хотите, можете сделать что-нибудь посложнее.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "    \n",
    "    \n",
    "    def _find_optimal_gamma(self, y: np.ndarray or list, old_predictions: np.ndarray,\n",
    "                            new_predictions: np.ndarray, boundaries: tuple or list=(0.01, 1)):\n",
    "        \"\"\"\n",
    "        Функция для поиска оптимального значения параметра gamma (коэффициент перед новой базовой моделью).\n",
    "        :param y: вектор целевой переменной\n",
    "        :param old_predictions: вектор суммы предсказаний предыдущих моделей (до сигмоиды)\n",
    "        :param new_predictions: вектор суммы предсказаний новой модели (после сигмоиды)\n",
    "        :param boudnaries: в каком диапазоне искать оптимальное значение ɣ (array-like объект из левой и правой границ)\n",
    "        \"\"\"\n",
    "        # Определеяем начальные лосс и оптимальную гамму\n",
    "        loss, optimal_gamma = self.loss_fn(y, old_predictions), 0\n",
    "        # Множество, на котором будем искать оптимальное значение гаммы\n",
    "        gammas = np.linspace(*boundaries, 100)\n",
    "        # Простым перебором ищем оптимальное значение\n",
    "        for gamma in gammas:\n",
    "            predictions = old_predictions + gamma * new_predictions\n",
    "            gamma_loss = self.loss_fn(y, predictions)\n",
    "            if gamma_loss < loss:\n",
    "                optimal_gamma = gamma\n",
    "                loss = gamma_loss\n",
    "        \n",
    "        return optimal_gamma\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, eval_set=None):\n",
    "        \"\"\"\n",
    "        Функция для обучения всей модели бустинга\n",
    "        :param X: матрица признаков\n",
    "        :param y: вектор целевой переменной\n",
    "        :eval_set: кортеж (X_val, y_val) для контроля процесса обучения или None, если контроль не используется\n",
    "        \"\"\"\n",
    "            \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "        \n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Функция для предсказания классов обученной моделью бустинга\n",
    "        :param X: матрица признаков\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Функция для предсказания вероятностей классов обученной моделью бустинга\n",
    "        :param X: матрица признаков\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        \"\"\"\n",
    "        Для бонусного задания номер 5.\n",
    "        Функция для вычисления важностей признаков.\n",
    "        Вычисление должно проводиться после обучения модели\n",
    "        и быть доступно атрибутом класса. \n",
    "        \"\"\"\n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тест для вашей имплементации. Если класс написан правильно, две следующие ячейки должна отработать без ошибок и относительно быстро (у автора задания 2 и 0.2 секунд соответственно, accuracy 0.911 и 0.879 соответственно). Если у вас получилось качество выше указанного — отлично!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "boosting = Boosting()\n",
    "boosting.fit(X_train_synthetic, y_train_synthetic)\n",
    "# Без разницы, выдает эта строка классы или вероятности\n",
    "preds = np.round(boosting.predict(X_test_synthetic) > 0.5)\n",
    "assert accuracy_score((y_test_synthetic == 1), np.round(preds)) > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "boosting = Boosting()\n",
    "boosting.fit(df_train.select_dtypes(['int64', 'float64']).drop(columns='y').values, df_train.y.values)\n",
    "# Без разницы, выдает эта строка классы или вероятности\n",
    "preds = np.round(boosting.predict(df_test.select_dtypes(['int64', 'float64']).drop(columns='y').values) > 0.5)\n",
    "assert accuracy_score((df_test.y.values == 1), np.round(preds)) > 0.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Сравните результаты вашей имплементации бустинга с указанными ниже базовыми моделями на обоих датасетах и ответьте на вопросы. Разумеется, надо измерять качество на тестовых данных. \n",
    "\n",
    "Варианты для базовой модели (разумеется, не надо их программировать самостоятельно, берите нужные классы из sklearn):\n",
    "\n",
    "- Решающее дерево глубины 6\n",
    "- Случайный лес (число деревьев — на ваше усмотрение, только не слишком мало)\n",
    "- Линейная регрессия\n",
    "\n",
    "Вопросы:\n",
    "\n",
    "1) Какая из моделей имеет оптимальное качество? С чем это связано?\n",
    "\n",
    "2) Какая из моделей сильнее переобучается? Есть ли преимущества от использования ранней остановки и обрезания бустинга до лучшей модели?\n",
    "\n",
    "3) Работает ли бустинг над линейными регрессиями лучше, чем одна логистическая регрессия? Как объяснить этот результат?\n",
    "\n",
    "4) Визуализируйте предсказания моделей на синтетическом датасете (для этого можете воспользоваться вспомогательной функцией plot_predicts). Чем отличаются картинки, которые получаются у разных алгоритмов? Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Мы разобрались с бустингом, теперь интересно посмотреть на комбинации моделей. Сравните результаты следующих моделей на обоих датасетах и ответьте на вопросы. Разумеется, надо измерять качество на тестовых данных.\n",
    "\n",
    "Используйте логистическую регрессию, случайный лес и BaggingClassifier из sklearn.\n",
    "\n",
    "- Случайный лес\n",
    "- Бэггинг на деревьях (поставьте для базовых деревьев min_samples_leaf=1)\n",
    "- Бэггинг на деревьях с обучением каждого дерева на подмножестве признаков (`max_features` около 0.6 в BaggingClassifier)\n",
    "- Бэггинг, у которого базовой моделью является бустинг с большим числом деревьев (> 100)\n",
    "- Бэггинг на логистических регрессиях\n",
    "\n",
    "1) Какая из моделей имеет лучшее качество? С чем это связано?\n",
    "\n",
    "2) Какая из моделей сильнее всего переобучается? Помогает ли бустингу ранняя остановка? \n",
    "\n",
    "3) Исправляет ли бэггинг переобученность бустинга с большим числом деревьев?\n",
    "\n",
    "4) Что лучше: случайный лес или бэггинг на деревьях с сэмплированием признаков?\n",
    "\n",
    "5) Если использовать деревья в качестве базового алгоритма, что лучше — бэггинг или бустинг? С чем это связано?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Сравните на этих данных любую из трёх популярных имплементаций градиентного бустинга (xgboost, lightgbm, catboost) с вашей реализацией. Подберите основные гиперпараметры (число деревьев, длина шага, глубина дерева/число листьев) для обоих методов. Получилось ли у вас победить библиотечные реализации на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "492px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
